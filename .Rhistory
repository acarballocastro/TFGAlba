anova(model)
library(mixlm)
resultats <- lm(reduc ~ edad + farmac)
Anova(resultats, type="III")
var = Anova(resultats, type="III")$var.comps
var = Anova(resultats, type="II")$var.comps
var
var = Anova(resultats, type="III")$var.comps
var
Anova(resultats, type="II")
Anova(resultats, type="I")
Anova(resultats, type="III")
resultats <- lm(reduc ~ edad + farmac)
Anova(resultats, type="III")
reduc <- c(7.08, 7.15, 9.91, 10.63, 10.21, 9.37, 15.05, 12.81, 6.61, 9.78, 11.61, 12.91)
edad <- factor(rep(rep(1:4),3), labels = c("<30","40-50", "50-60",">60"))
farmac <- factor(rep(1:3, each = 4), labels = c("A", "B", "C"))
data.frame(reduc, edad, farmac)
model <- aov(reduc ~ edad + farmac)
anova(model)
library(mixlm)
resultats <- lm(reduc ~ edad + farmac)
Anova(resultats, type="III")
var = Anova(resultats, type="III")$var.comps
var
res<-resid(model)
shapiro.test((res))
desgast <- c(0.3, 2.1, 4.2, 4, 3.8, 5, 1.5, 3.9, 4.3, 2.3, 5.1, 4.7, 4.6, 5, 5.1, 5.7, 6.3, 5.8)
presion <- factor(rep(rep(1:3,each=2),3), labels = c("300", "350", "400"))
presion
temp <- factor(rep(1:3, each = 6), labels = c("10", "15", "20"))
temp
data.frame(desgast, presion, temp)
model <- aov(desgast ~ presion*temp)
anova(model)
resultats <- mixlm::lm(desgast ~ presion*temp)
Anova(resultats, type="III")
res<-resid(model)
shapiro.test((res))
bartlett.test(res ~ presion)
bartlett.test(res ~ temp)
viscositat <- c(25.6, 22.5, 33.6, 30.7, 29.1, 33.5, 21.2, 22.9, 20, 20.1, 26.8,
25.8, 26.5, 26.7, 26.4, 23.1, 36.9, 33.7, 45.9, 45.2, 43.4, 44.4,
36.2, 37.6)
lot <- factor(rep(rep(1:4,each=2),3))
lot
metode <- factor(rep(1:3, each = 8), labels = c("A", "B", "C"))
metode
data.frame(viscositat, lot, metode)
resultats <- mixlm::lm(viscositat ~ r(lot)*metode, unrestricted = TRUE)
Anova(resultats, type="III")
tabla = Anova(resultats, type="III")$anova
tabla[2,5]
mesura <- c(2003, 1999, 2000, 2007, 2008, 2004, 1999, 1999, 2004, 1996, 1994,
1994, 2009, 2009, 2005, 1993, 1991, 1995, 1996, 2003, 2000, 2005,
2004, 2013, 2003, 1999, 1999)
length(mesura)
cargol <- factor(rep(rep(1:3,each=3),3))
cargol
vascula <- factor(rep(1:3, each = 9))
vascula
data.frame(mesura, cargol, vascula)
resultats <- mixlm::lm(mesura ~ r(cargol)*r(vascula))
Anova(resultats, type="III")
Anova(resultats, type="III")$var.comps
rendiment <- c(66, 64, 63, 57, 60, 56, 62, 62, 58, 54, 73, 72, 42, 39, 47, 42,
39, 40, 47, 35, 44, 48, 51, 51, 31, 40, 44, 34, 34, 36, 25, 36,
43, 30, 54, 46)
centre <- factor(rep(1:3, each=12))
centre
entrenador <- factor(rep(1:3,each=4,3))
entrenador
data.frame(centre, entrenador)
resultats <- mixlm::lm(rendiment ~ r(centre) + r(entrenador)%in%r(centre))
Anova(resultats, type="III")
sum(Anova(resultats, type="III")$var.comps)
comb <- interaction(presion, temp)
# bartlett.test(res ~ presion)
# bartlett.test(res ~ temp)
bartlett.test(res ~ comb)
install.packages("flexdashboard")
library(flexdashboard)
library(renv)
install.packages("renv")
library(flexdashboard)
library(tidyverse)
library(readr)
read_csv("https://raw.githubusercontent.com/fivethirtyeight/data/master/alcohol-consumption/drinks.csv")
datos1 = read_csv("https://raw.githubusercontent.com/fivethirtyeight/data/master/alcohol-consumption/drinks.csv")
View(datos1)
datos1
datos12 = read_csv("https://raw.githubusercontent.com/fivethirtyeight/data/master/alcohol-consumption/drinks.csv", cols = (country = col_factor(), .default = col_integer(),
total_litres_of_pure_alcohol = col_double()))
datos12 = read_csv("https://raw.githubusercontent.com/fivethirtyeight/data/master/alcohol-consumption/drinks.csv", col_types = cols(.default = col_integer(), country = col_factor(), total_litres_of_pure_alcohol = col_double()))
datos12
install.packages("installr")
library(installr)
updateR()
updateR()
list.of.packages <- c("tidyverse", "nycflights13")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
library(tidyverse)
library(nycflights13)
data(flights, package = "nycflights13")
flights
head(flights)
flights %>%
filter(arr_time > 2*60)
flights %>%
filter(arr_delay > 2*60)
flights %>%
filter(origin %in% c("IAH", "HOU"))
flights
View(flights)
View(flights)
flights %>%
filter(dest %in% c("IAH", "HOU"))
flights %>%
filter(carrier %in% c("AA", "DL", "UA"))
flights %>%
filter(month %in% c(7,8,9))
View(flights)
flights %>%
filter(dep_delay == 0 $& arr_delay > 2*60)
flights %>%
filter(dep_delay == 0 & arr_delay > 2*60)
flights %>%
filter(between(dep_time, 0, 600))
flights %>%
arrange(., desc(distance))
flights %>%
arrange(., desc(distance))
flights %>%
arrange(distance)
flights %>%
arrange(distance) %>%
slice(1:10)
flights %>%
arrange(arr_delay) %>%
slice(1:10)
flights %>%
arrange(arr_delay, desc()) %>%
slice(1:10)
flights %>%
arrange(arr_delay, desc(arr_delay)) %>%
slice(1:10)
flights %>%
arrange(arr_delay, desc(arr_delay))
flights %>%
arrange(desc(arr_delay)) %>%
slice(1:10)
flights %>%
arrange(desc(arr_delay)) %>%
slice(1:5)
flights %>%
arrange(desc(arr_delay)) %>%
head(5)
flights %>%
arrange(desc(air_time/distance)) %>%
head(5) # Otra forma para seleccionar los 5 primeros
flights %>%
arrange(air_time/distance) %>%
head(5) # Otra forma para seleccionar los 5 primeros
flights %>%
arrange(desc(air_time/distance)) %>%
head() # Otra forma para seleccionar los 5 primeros
flights %>%
arrange(desc(air_time/distance)) %>%
head(1) # Otra forma para seleccionar los 5 primeros
flights %>%
arrange(!is.na(dep_time), dep_time)
flights
dep_time/100
flights$dep_time/100
floor(flights$dep_time/100)
flights %>%
mutate(dep_time_mid = floor(dep_time/100)*60)
flights %>%
mutate(dep_time_mid = floor(dep_time/100)*60 + (dep_time - floor(dep_time/100)*100))
flights$dep_time_mid
flights %>%
transmute(dep_time_mid = floor(dep_time/100)*60 + (dep_time - floor(dep_time/100)*100))
flights
flights %>%
transmute(dep_time_mid = floor(dep_time/100)*60 + (dep_time - floor(dep_time/100)*100),
sched_dep_time_mid = floor(sched_dep_time/100)*60 + (sched_dep_time - floor(sched_dep_time/100)*100))
library(tidyverse)
library(nycflights13)
data(flights, package = "nycflights13")
head(flights)
View(flights)
View(flights)
flights %>%
mutate(sched_dep_time_mid = hour*60 + minute,
dep_time_mid = hour*60 + minute + dep_delay)
flights %>%
mutate(sched_dep_time_mid = hour*60 + minute,
dep_time_mid = hour*60 + minute + dep_delay) %>%
select(dep_time, dep_time_mid, sched_dep_time, sched_dep_time_mid, hour, minute, dep_delay)
flights %>%
mutate(dep_time_mid = floor(dep_time/100)*60 + (dep_time - floor(dep_time/100)*100),
sched_dep_time_mid = floor(sched_dep_time/100)*60 + (sched_dep_time - floor(sched_dep_time/100)*100)) %>%
select(dep_time, dep_time_mid, sched_dep_time, sched_dep_time_mid)
flights %>% group_by(day) %>%
first(dep_time)
flights %>% group_by(day, dep_time) %>%
first(dep_time)
flights %>% group_by(day) %>%
summartise(primer_vuelo = first(dep_time),
ultimo_vuelo = last(dep_time))
flights %>% group_by(day) %>%
summarise(primer_vuelo = first(dep_time),
ultimo_vuelo = last(dep_time))
flights %>% group_by(day) %>%
summarise(primer_vuelo = first(!is.na(dep_time)),
ultimo_vuelo = last(!is.na(dep_time)))
flights %>% group_by(day) %>%
summarise(primer_vuelo = first(na.rm(dep_time)),
ultimo_vuelo = last(!is.na(dep_time)))
flights %>% group_by(day) %>%
summarise(primer_vuelo = first(dep_time, na.rm = T),
ultimo_vuelo = last(!is.na(dep_time)))
flights %>% group_by(day) %>%
summarise(primer_vuelo = first(dep_time),
ultimo_vuelo = last(dep_time))
no_cancelados %>% group_by(day) %>%
summarise(primer_vuelo = first(dep_time),
ultimo_vuelo = last(dep_time))
no_cancelados <- flights %>%
filter(!is.na(dep_delay), !is.na(arr_delay))
no_cancelados %>% group_by(day) %>%
summarise(primer_vuelo = first(dep_time),
ultimo_vuelo = last(dep_time))
flights %>%
filter(!is.na(dep_delay), !is.na(arr_delay)) %>%
group_by(day) %>%
summarise(primer_vuelo = first(dep_time),
ultimo_vuelo = last(dep_time))
flights %>%
group_by(dest) %>%
count(carrier)
flights %>%
group_by(dest, carrier) %>%
summarise(vuelos = n())
flights %>%
group_by(dest, carrier) %>%
summarise(vuelos = n())
flights %>%
group_by(dest) %>%
summarise(vuelos = count(carrier))
flights %>%
group_by(dest) %>%
summarise(vuelos = n(carrier))
flights %>%
group_by(dest) %>%
count(carrier)
flights %>%
group_by(dest) %>%
count(carrier) %>%
count(dest)
flights %>%
group_by(dest) %>%
count(carrier)
flights %>%
group_by(dest) %>%
count(carrier) %>%
count(dest) %>% arrange()
flights %>%
group_by(dest) %>%
count(carrier) %>%
count(dest) %>% arrange(n)
flights %>%
group_by(dest) %>%
count(carrier) %>%
count(dest) %>% arrange(n, desc(n))
flights %>%
group_by(dest) %>%
count(carrier) %>%
count(dest) %>% arrange(desc(n))
(retraso_salida <- flights %>%
group_by(dest) %>%
summarise(num_vuelos = n(),
dist_media = mean(distance, na.rm = TRUE),
ret_medio = mean(dep_delay, na.rm = TRUE)
) %>%
filter(num_vuelos > 20, dest != "HNL"))
retraso_salida %>%
ggplot(aes(x = dist_media, y = ret_medio)) +
geom_point(aes(size = num_vuelos), alpha = 1/3) +
geom_smooth(se = FALSE)
(retraso_salida <- flights %>%
group_by(dest) %>%
summarise(num_vuelos = n(),
dist_media = mean(distance, na.rm = TRUE),
ret_medio_salida = mean(dep_delay, na.rm = TRUE)
) %>%
filter(num_vuelos > 20, dest != "HNL"))
retraso_salida %>%
ggplot(aes(x = dist_media, y = ret_medio_salida)) +
geom_point(aes(size = num_vuelos), alpha = 1/3) +
geom_smooth(se = FALSE)
library(tidyverse)
library(forecats) # Dentro de tidyverse
library(forcats) # Dentro de tidyverse
gss_cat
head(gss_cat) # Datos
gss_cat %>%
count(relig) %>% # Hacemos que cuente las religiones
arrange(desc(n)) %>% # Ordenamos descendentemente
head(1)
gss_cat %>%
count(relig) %>% # Hacemos que cuente las religiones
arrange(desc(n)) %>% # Ordenamos descendentemente
first()
gss_cat %>%
count(relig) %>% # Hacemos que cuente las religiones
arrange(desc(n))
gss_cat %>%
count(relig) %>% # Hacemos que cuente las religiones
arrange(desc(n)) %>% # Ordenamos descendentemente
slice_head()
gss_cat %>%
count(relig) %>% # Hacemos que cuente las religiones
arrange(desc(n)) %>% # Ordenamos descendentemente
slice_head() # head(1) también habría valido
gss_cat %>%
count(partyid) %>% # Hacemos que cuente las religiones
arrange(desc(n)) %>% # Ordenamos descendentemente
slice_head()
gss_cat <- gss_cat %>%
select_if(is.factor)
gss_cat %>%
select_if(is.factor)
head(gss_cat) # Datos
library(tidyverse) # forcats esta dentro
head(gss_cat) # Datos
library(tidyverse) # forcats esta dentro
head(gss_cat) # Datos
library(tidyverse) # forcats esta dentro
head(gss_cat) # Datos
gss_cat %>%
select_if(is.factor)
gss_cat %>%
select_if(is.factor) %>%
colnames()
# Marital
levels(gss_cat[["marital"]])
gss_cat %>%
select(tvhours) %>%
summary
head(gss_cat) # Datos
str(gss_cat)
gss_cat %>%
select(tvhours) %>%
summary
gss_cat %>%
select(tvhours) %>% mean
gss_cat %>%
select(tvhours) %>% summarise(mean)
gss_cat %>%
summarise(promedio = mean(tvhours))
gss_cat %>%
summarise(promedio = mean(tvhours))
gss_cat %>%
summarise(promedio = mean(tvhours, na.rm = T))
gss_cat %>%
select(tvhours) %>%
summary
# Dibujamos la distribución
gss_cat %>%
filter(!is.na(tvhours)) %>%
ggplot(aes(x = tvhours)) +
geom_histogram(binwidth = 1)
# Dibujamos la distribución
gss_cat %>%
ggplot(aes(x = !is.na(tvhours))) +
geom_histogram(binwidth = 1)
# Dibujamos la distribución
gss_cat %>%
filter(!is.na(tvhours)) %>%
ggplot(aes(x = !is.na(tvhours))) +
geom_histogram(binwidth = 1)
# Dibujamos la distribución
gss_cat %>%
filter(!is.na(tvhours)) %>%
ggplot(aes(x = tvhours)) +
geom_histogram(binwidth = 1)
# Dibujamos la distribución
gss_cat %>%
filter(!is.na(tvhours)) %>% # Filtramos los NA
ggplot(aes(x = tvhours)) +
geom_histogram()
# Dibujamos la distribución
gss_cat %>%
filter(!is.na(tvhours)) %>% # Filtramos los NA
ggplot(aes(x = tvhours)) +
geom_histogram(bins = 24)
gss_cat %>% filter(partyid)
gss_cat %>% select(partyid)
gss_cat %>% select(partyid) %>%
levels
gss_cat %>% select(partyid) %>%
levels()
gss_cat %>% filter(!is.na(partyid)) %>%
count(partyid, sort = TRUE)
# Usamos fct_collapse para crear categorías
# Democrat: "Not str democrat", "Strong democrat"
# Republican: "Strong republican", "Not str republican"
# Independent: "Ind,near rep", "Independent", "Ind,near dem"
gss_cat %>%
# Usamos fct_collapse para crear categorías
# Democrat: "Not str democrat", "Strong democrat"
# Republican: "Strong republican", "Not str republican"
# Independent: "Ind,near rep", "Independent", "Ind,near dem"
gss_cat %>%
gss_cat %>%
mutate(partyid = fct_collapse(partyid, dem = c("Not str democrat", "Strong democrat"),
rep = c("Strong republican", "Not str republican"),
ind = c("Ind,near rep", "Independent", "Ind,near dem"))) %>%
count(year, partyid) %>% # Seleccionamos por año
group_by(year) %>% # Agurpamos por año
mutate(prop = n / sum(n)) %>% # Calculamos la proporción
ggplot(aes(
x = year, y = p,
colour = fct_reorder2(partyid, year, p)
)) +
geom_point() +
geom_line() +
labs(colour = "Party ID."
# Usamos fct_collapse para crear categorías
# Democrat: "Not str democrat", "Strong democrat"
# Republican: "Strong republican", "Not str republican"
# Independent: "Ind,near rep", "Independent", "Ind,near dem"
gss_cat %>%
mutate(partyid = fct_collapse(partyid, dem = c("Not str democrat", "Strong democrat"),
rep = c("Strong republican", "Not str republican"),
ind = c("Ind,near rep", "Independent", "Ind,near dem"))) %>%
count(year, partyid) %>% # Seleccionamos por año
group_by(year) %>% # Agurpamos por año
mutate(prop = n / sum(n)) %>% # Calculamos la proporción
ggplot(aes(
x = year, y = p,
colour = fct_reorder2(partyid, year, p)
)) +
geom_point() +
geom_line() +
labs(colour = "Party ID.")
library(tidyverse)
library(ggpubr)
library(shapr)
library(caret)
library(randomForest)
dataset <- read_csv("data/datasetADNI.csv")
dataset <- dataset[,-1] # Removing ID
setwd("C:/Users/albac/Desktop/MatEst/TFG/TFGAlba")
dataset <- read_csv("data/datasetADNI.csv")
dataset <- dataset[,-1] # Removing ID
head(dataset)
dataset$APOE4 = as.factor(dataset$APOE4)
dataset$PTGENDER = as.factor(dataset$PTGENDER)
dataset$DX = as.factor(dataset$DX)
head(dataset)
# Fixing covariables and response variable
x_var <- c("FDG","ABETA","PTAU","APOE4","PTGENDER","AGE","PTEDUCAT")
y_var <- "DX"
# Splitting in train-test (80%-20%) ----
set.seed(2022)
train_index <- caret::createDataPartition(dataset$DX, p = .8, list = FALSE, times = 1)
# Training data
x_train <- as.matrix(dataset[train_index, x_var])
y_train_nc <- as.matrix(dataset[train_index, y_var]) # not centered
y_train <- y_train_nc - mean(y_train_nc) # Tengo covariables categóricas
# Random forest ----
model <- randomForest(x = x_train, y = y_train_nc, ntree=500)
# Random forest ----
model <- randomForest(x = x_train_nc, y = y_train_nc, ntree=500)
# Random forest ----
model <- randomForest(x = x_train, y = y_train_nc, ntree=500)
head(dataset)
# Training data
x_train <- as.matrix(dataset[train_index, x_var])
x_train
# Random forest ----
model <- randomForest(x = x_train[,-5], y = y_train_nc, ntree=500)
y_train_nc
unclass(dataset$DX)
pull(unclass(dataset$DX))
head(dataset)
dataset$APOE4 = unclass(dataset$APOE4)
dataset$PTGENDER = unclass(dataset$PTGENDER)
dataset$DX = unclass(dataset$DX)
head(dataset)
# Fixing covariables and response variable
x_var <- c("FDG","ABETA","PTAU","APOE4","PTGENDER","AGE","PTEDUCAT")
y_var <- "DX"
# Splitting in train-test (80%-20%) ----
set.seed(2022)
train_index <- caret::createDataPartition(dataset$DX, p = .8, list = FALSE, times = 1)
# Training data
x_train <- as.matrix(dataset[train_index, x_var])
y_train_nc <- as.matrix(dataset[train_index, y_var]) # not centered
y_train <- y_train_nc - mean(y_train_nc) # Tengo covariables categóricas
# Test data
x_test <- as.matrix(dataset[-train_index, x_var])
y_test_nc <- as.matrix(dataset[-train_index, y_var]) # not centered
y_test <- y_test_nc - mean(y_train_nc) # Tengo covariables categóricas
# Random forest ----
model <- randomForest(x = x_train, y = y_train_nc, ntree=500)
print(model)
explainer_symmetric <- shapr(x_train, model)
head(dataset)
