filter(dep_delay == 0 & arr_delay > 2*60)
flights %>%
filter(between(dep_time, 0, 600))
flights %>%
arrange(., desc(distance))
flights %>%
arrange(., desc(distance))
flights %>%
arrange(distance)
flights %>%
arrange(distance) %>%
slice(1:10)
flights %>%
arrange(arr_delay) %>%
slice(1:10)
flights %>%
arrange(arr_delay, desc()) %>%
slice(1:10)
flights %>%
arrange(arr_delay, desc(arr_delay)) %>%
slice(1:10)
flights %>%
arrange(arr_delay, desc(arr_delay))
flights %>%
arrange(desc(arr_delay)) %>%
slice(1:10)
flights %>%
arrange(desc(arr_delay)) %>%
slice(1:5)
flights %>%
arrange(desc(arr_delay)) %>%
head(5)
flights %>%
arrange(desc(air_time/distance)) %>%
head(5) # Otra forma para seleccionar los 5 primeros
flights %>%
arrange(air_time/distance) %>%
head(5) # Otra forma para seleccionar los 5 primeros
flights %>%
arrange(desc(air_time/distance)) %>%
head() # Otra forma para seleccionar los 5 primeros
flights %>%
arrange(desc(air_time/distance)) %>%
head(1) # Otra forma para seleccionar los 5 primeros
flights %>%
arrange(!is.na(dep_time), dep_time)
flights
dep_time/100
flights$dep_time/100
floor(flights$dep_time/100)
flights %>%
mutate(dep_time_mid = floor(dep_time/100)*60)
flights %>%
mutate(dep_time_mid = floor(dep_time/100)*60 + (dep_time - floor(dep_time/100)*100))
flights$dep_time_mid
flights %>%
transmute(dep_time_mid = floor(dep_time/100)*60 + (dep_time - floor(dep_time/100)*100))
flights
flights %>%
transmute(dep_time_mid = floor(dep_time/100)*60 + (dep_time - floor(dep_time/100)*100),
sched_dep_time_mid = floor(sched_dep_time/100)*60 + (sched_dep_time - floor(sched_dep_time/100)*100))
library(tidyverse)
library(nycflights13)
data(flights, package = "nycflights13")
head(flights)
View(flights)
View(flights)
flights %>%
mutate(sched_dep_time_mid = hour*60 + minute,
dep_time_mid = hour*60 + minute + dep_delay)
flights %>%
mutate(sched_dep_time_mid = hour*60 + minute,
dep_time_mid = hour*60 + minute + dep_delay) %>%
select(dep_time, dep_time_mid, sched_dep_time, sched_dep_time_mid, hour, minute, dep_delay)
flights %>%
mutate(dep_time_mid = floor(dep_time/100)*60 + (dep_time - floor(dep_time/100)*100),
sched_dep_time_mid = floor(sched_dep_time/100)*60 + (sched_dep_time - floor(sched_dep_time/100)*100)) %>%
select(dep_time, dep_time_mid, sched_dep_time, sched_dep_time_mid)
flights %>% group_by(day) %>%
first(dep_time)
flights %>% group_by(day, dep_time) %>%
first(dep_time)
flights %>% group_by(day) %>%
summartise(primer_vuelo = first(dep_time),
ultimo_vuelo = last(dep_time))
flights %>% group_by(day) %>%
summarise(primer_vuelo = first(dep_time),
ultimo_vuelo = last(dep_time))
flights %>% group_by(day) %>%
summarise(primer_vuelo = first(!is.na(dep_time)),
ultimo_vuelo = last(!is.na(dep_time)))
flights %>% group_by(day) %>%
summarise(primer_vuelo = first(na.rm(dep_time)),
ultimo_vuelo = last(!is.na(dep_time)))
flights %>% group_by(day) %>%
summarise(primer_vuelo = first(dep_time, na.rm = T),
ultimo_vuelo = last(!is.na(dep_time)))
flights %>% group_by(day) %>%
summarise(primer_vuelo = first(dep_time),
ultimo_vuelo = last(dep_time))
no_cancelados %>% group_by(day) %>%
summarise(primer_vuelo = first(dep_time),
ultimo_vuelo = last(dep_time))
no_cancelados <- flights %>%
filter(!is.na(dep_delay), !is.na(arr_delay))
no_cancelados %>% group_by(day) %>%
summarise(primer_vuelo = first(dep_time),
ultimo_vuelo = last(dep_time))
flights %>%
filter(!is.na(dep_delay), !is.na(arr_delay)) %>%
group_by(day) %>%
summarise(primer_vuelo = first(dep_time),
ultimo_vuelo = last(dep_time))
flights %>%
group_by(dest) %>%
count(carrier)
flights %>%
group_by(dest, carrier) %>%
summarise(vuelos = n())
flights %>%
group_by(dest, carrier) %>%
summarise(vuelos = n())
flights %>%
group_by(dest) %>%
summarise(vuelos = count(carrier))
flights %>%
group_by(dest) %>%
summarise(vuelos = n(carrier))
flights %>%
group_by(dest) %>%
count(carrier)
flights %>%
group_by(dest) %>%
count(carrier) %>%
count(dest)
flights %>%
group_by(dest) %>%
count(carrier)
flights %>%
group_by(dest) %>%
count(carrier) %>%
count(dest) %>% arrange()
flights %>%
group_by(dest) %>%
count(carrier) %>%
count(dest) %>% arrange(n)
flights %>%
group_by(dest) %>%
count(carrier) %>%
count(dest) %>% arrange(n, desc(n))
flights %>%
group_by(dest) %>%
count(carrier) %>%
count(dest) %>% arrange(desc(n))
(retraso_salida <- flights %>%
group_by(dest) %>%
summarise(num_vuelos = n(),
dist_media = mean(distance, na.rm = TRUE),
ret_medio = mean(dep_delay, na.rm = TRUE)
) %>%
filter(num_vuelos > 20, dest != "HNL"))
retraso_salida %>%
ggplot(aes(x = dist_media, y = ret_medio)) +
geom_point(aes(size = num_vuelos), alpha = 1/3) +
geom_smooth(se = FALSE)
(retraso_salida <- flights %>%
group_by(dest) %>%
summarise(num_vuelos = n(),
dist_media = mean(distance, na.rm = TRUE),
ret_medio_salida = mean(dep_delay, na.rm = TRUE)
) %>%
filter(num_vuelos > 20, dest != "HNL"))
retraso_salida %>%
ggplot(aes(x = dist_media, y = ret_medio_salida)) +
geom_point(aes(size = num_vuelos), alpha = 1/3) +
geom_smooth(se = FALSE)
library(tidyverse)
library(forecats) # Dentro de tidyverse
library(forcats) # Dentro de tidyverse
gss_cat
head(gss_cat) # Datos
gss_cat %>%
count(relig) %>% # Hacemos que cuente las religiones
arrange(desc(n)) %>% # Ordenamos descendentemente
head(1)
gss_cat %>%
count(relig) %>% # Hacemos que cuente las religiones
arrange(desc(n)) %>% # Ordenamos descendentemente
first()
gss_cat %>%
count(relig) %>% # Hacemos que cuente las religiones
arrange(desc(n))
gss_cat %>%
count(relig) %>% # Hacemos que cuente las religiones
arrange(desc(n)) %>% # Ordenamos descendentemente
slice_head()
gss_cat %>%
count(relig) %>% # Hacemos que cuente las religiones
arrange(desc(n)) %>% # Ordenamos descendentemente
slice_head() # head(1) también habría valido
gss_cat %>%
count(partyid) %>% # Hacemos que cuente las religiones
arrange(desc(n)) %>% # Ordenamos descendentemente
slice_head()
gss_cat <- gss_cat %>%
select_if(is.factor)
gss_cat %>%
select_if(is.factor)
head(gss_cat) # Datos
library(tidyverse) # forcats esta dentro
head(gss_cat) # Datos
library(tidyverse) # forcats esta dentro
head(gss_cat) # Datos
library(tidyverse) # forcats esta dentro
head(gss_cat) # Datos
gss_cat %>%
select_if(is.factor)
gss_cat %>%
select_if(is.factor) %>%
colnames()
# Marital
levels(gss_cat[["marital"]])
gss_cat %>%
select(tvhours) %>%
summary
head(gss_cat) # Datos
str(gss_cat)
gss_cat %>%
select(tvhours) %>%
summary
gss_cat %>%
select(tvhours) %>% mean
gss_cat %>%
select(tvhours) %>% summarise(mean)
gss_cat %>%
summarise(promedio = mean(tvhours))
gss_cat %>%
summarise(promedio = mean(tvhours))
gss_cat %>%
summarise(promedio = mean(tvhours, na.rm = T))
gss_cat %>%
select(tvhours) %>%
summary
# Dibujamos la distribución
gss_cat %>%
filter(!is.na(tvhours)) %>%
ggplot(aes(x = tvhours)) +
geom_histogram(binwidth = 1)
# Dibujamos la distribución
gss_cat %>%
ggplot(aes(x = !is.na(tvhours))) +
geom_histogram(binwidth = 1)
# Dibujamos la distribución
gss_cat %>%
filter(!is.na(tvhours)) %>%
ggplot(aes(x = !is.na(tvhours))) +
geom_histogram(binwidth = 1)
# Dibujamos la distribución
gss_cat %>%
filter(!is.na(tvhours)) %>%
ggplot(aes(x = tvhours)) +
geom_histogram(binwidth = 1)
# Dibujamos la distribución
gss_cat %>%
filter(!is.na(tvhours)) %>% # Filtramos los NA
ggplot(aes(x = tvhours)) +
geom_histogram()
# Dibujamos la distribución
gss_cat %>%
filter(!is.na(tvhours)) %>% # Filtramos los NA
ggplot(aes(x = tvhours)) +
geom_histogram(bins = 24)
gss_cat %>% filter(partyid)
gss_cat %>% select(partyid)
gss_cat %>% select(partyid) %>%
levels
gss_cat %>% select(partyid) %>%
levels()
gss_cat %>% filter(!is.na(partyid)) %>%
count(partyid, sort = TRUE)
# Usamos fct_collapse para crear categorías
# Democrat: "Not str democrat", "Strong democrat"
# Republican: "Strong republican", "Not str republican"
# Independent: "Ind,near rep", "Independent", "Ind,near dem"
gss_cat %>%
# Usamos fct_collapse para crear categorías
# Democrat: "Not str democrat", "Strong democrat"
# Republican: "Strong republican", "Not str republican"
# Independent: "Ind,near rep", "Independent", "Ind,near dem"
gss_cat %>%
gss_cat %>%
mutate(partyid = fct_collapse(partyid, dem = c("Not str democrat", "Strong democrat"),
rep = c("Strong republican", "Not str republican"),
ind = c("Ind,near rep", "Independent", "Ind,near dem"))) %>%
count(year, partyid) %>% # Seleccionamos por año
group_by(year) %>% # Agurpamos por año
mutate(prop = n / sum(n)) %>% # Calculamos la proporción
ggplot(aes(
x = year, y = p,
colour = fct_reorder2(partyid, year, p)
)) +
geom_point() +
geom_line() +
labs(colour = "Party ID."
# Usamos fct_collapse para crear categorías
# Democrat: "Not str democrat", "Strong democrat"
# Republican: "Strong republican", "Not str republican"
# Independent: "Ind,near rep", "Independent", "Ind,near dem"
gss_cat %>%
mutate(partyid = fct_collapse(partyid, dem = c("Not str democrat", "Strong democrat"),
rep = c("Strong republican", "Not str republican"),
ind = c("Ind,near rep", "Independent", "Ind,near dem"))) %>%
count(year, partyid) %>% # Seleccionamos por año
group_by(year) %>% # Agurpamos por año
mutate(prop = n / sum(n)) %>% # Calculamos la proporción
ggplot(aes(
x = year, y = p,
colour = fct_reorder2(partyid, year, p)
)) +
geom_point() +
geom_line() +
labs(colour = "Party ID.")
install.packages("shinyWidgets")
shiny::runApp('C:/Users/albac/Desktop/MatEst/IAE/tarea_shiny_albcarcas1')
runApp('C:/Users/albac/Desktop/MatEst/IAE/tarea_shiny_albcarcas1')
runApp('C:/Users/albac/Desktop/MatEst/IAE/tarea_shiny_albcarcas1')
runApp('C:/Users/albac/Desktop/MatEst/IAE/tarea_shiny_albcarcas1')
runApp('C:/Users/albac/Desktop/MatEst/IAE/tarea_shiny_albcarcas1')
ruspini
data(ruspini)
library(ruspini)
library(hclust)
library(cluster)
ruspini
agriculture
# k-means only works with numerical variables,
# so don't give the user the option to select
# a categorical variable
data <- agriculture
runApp('C:/Users/albac/Desktop/MatEst/IAE/tarea_shiny_albcarcas1')
runApp('C:/Users/albac/Desktop/MatEst/IAE/tarea_shiny_albcarcas1')
USArrests
setdiff(names(iris), "Species")
names(USArrests)
runApp('C:/Users/albac/Desktop/MatEst/IAE/tarea_shiny_albcarcas1')
runApp('C:/Users/albac/Desktop/MatEst/IAE/tarea_shiny_albcarcas1')
runApp('C:/Users/albac/Desktop/MatEst/IAE/tarea_shiny_albcarcas1')
runApp('C:/Users/albac/Desktop/MatEst/IAE/tarea_shiny_albcarcas1')
runApp('C:/Users/albac/Desktop/MatEst/IAE/tarea_shiny_albcarcas1')
runApp('C:/Users/albac/Desktop/MatEst/IAE/tarea_shiny_albcarcas1')
plot(2:10, sapply(2:10,function(x) pam(USArrests,x)$silinfo$avg.width),
type = "l", xlab = "Number of medoids (k)",
ylab = "Average cluster width", main = "Average cluster width representation")
runApp('C:/Users/albac/Desktop/MatEst/IAE/tarea_shiny_albcarcas1')
runApp('C:/Users/albac/Desktop/MatEst/IAE/tarea_shiny_albcarcas1')
runApp('C:/Users/albac/Desktop/MatEst/IAE/tarea_shiny_albcarcas1')
pam = pam(data)
pam = pam(USArrests, 4)
pam$clustering
pam$clusinfo
runApp('C:/Users/albac/Desktop/MatEst/IAE/tarea_shiny_albcarcas1')
runApp('C:/Users/albac/Desktop/MatEst/IAE/tarea_shiny_albcarcas1')
runApp('C:/Users/albac/Desktop/MatEst/IAE/tarea_shiny_albcarcas1')
runApp('C:/Users/albac/Desktop/MatEst/IAE/tarea_shiny_albcarcas1')
runApp('C:/Users/albac/Desktop/MatEst/IAE/tarea_shiny_albcarcas1')
runApp('C:/Users/albac/Desktop/MatEst/IAE/tarea_shiny_albcarcas1/app2.R')
runApp('C:/Users/albac/Desktop/MatEst/IAE/tarea_shiny_albcarcas1/app3.R')
runApp('app3.R')
shiny::runApp('C:/Users/albac/Desktop/MatEst/IAE/tarea_shiny_albcarcas1')
install.packages("esquisse")
esquisse:::esquisser()
library(heplots)
data("RootStock")
summary(RootStock)
dim(RootStock)
R = cor(RootStock[,-1])
R
(autoval = eigen(R)$values)
round(autoval,2)
(autovec = eigen(R)$vectors)
(autoval = eigen(R)$values)
m = 2
L = autovec[,1:m]%*%diag(sqrt(autoval[1:m])) # Cargas factoriales
rownames(L)= colnames(RootStock[,-1])
colnames(L)= paste("Factor",1:m)
L # Matriz de cargas factoriales obtenida
(rotvarimax= varimax(L))
T = rotvarimax$rotmat # matriz de rotación
Lrot=unclass(rotvarimax$loadings)
#(Lrot= L%*%T)  #es lo mismo
colnames(Lrot)= c("F1 rot.","F2 rot.2")
Lrot
dataset <- read_csv("data/datasetADNI.csv")
library(tidyverse)
dataset <- read_csv("data/datasetADNI.csv")
setwd("C:/Users/albac/Desktop/MatEst/TFG/TFGAlba")
dataset <- read_csv("data/datasetADNI.csv")
head(dataset)
View(dataset)
sum(dataset$DXB)
library(tidyverse)
library(ggpubr)
library(rcompanion)
library(corrr)
library(shapr)
dataset <- read_csv("data/datasetADNI.csv")
dataset <- dataset[,c(-1,-9)] # Removing ID and DX variable
head(dataset)
mixed_assoc = function(df, cor_method="spearman", adjust_cramersv_bias=TRUE){
df_comb = expand.grid(names(df), names(df),  stringsAsFactors = F) %>% set_names("X1", "X2")
is_nominal = function(x) class(x) %in% c("factor", "character")
is_numeric <- function(x) { is.integer(x) || is_double(x)}
f = function(xName,yName) {
x =  pull(df, xName)
y =  pull(df, yName)
result = if(is_nominal(x) && is_nominal(y)){
# use bias corrected cramersV as described in https://rdrr.io/cran/rcompanion/man/cramerV.html
cv = cramerV(as.character(x), as.character(y), bias.correct = adjust_cramersv_bias)
data.frame(xName, yName, assoc=cv, type="cramersV")
}else if(is_numeric(x) && is_numeric(y)){
correlation = cor(x, y, method=cor_method, use="complete.obs")
data.frame(xName, yName, assoc=correlation, type="correlation")
}else if(is_numeric(x) && is_nominal(y)){
# from https://stats.stackexchange.com/questions/119835/correlation-between-a-nominal-iv-and-a-continuous-dv-variable/124618#124618
r_squared = summary(lm(x ~ y))$r.squared
data.frame(xName, yName, assoc=sqrt(r_squared), type="anova")
}else if(is_nominal(x) && is_numeric(y)){
r_squared = summary(lm(y ~x))$r.squared
data.frame(xName, yName, assoc=sqrt(r_squared), type="anova")
}else {
warning(paste("unmatched column type combination: ", class(x), class(y)))
}
# finally add complete obs number and ratio to table
result %>% mutate(complete_obs_pairs=sum(!is.na(x) & !is.na(y)), complete_obs_ratio=complete_obs_pairs/length(x)) %>% rename(x=xName, y=yName)
}
# apply function to each variable combination
map2_df(df_comb$X1, df_comb$X2, f)
}
mixed_assoc(dataset)
assocs <- dataset %>%
#select(-DXB) %>%
mixed_assoc() %>%
select(x, y, assoc) %>%
spread(y, assoc) %>%
column_to_rownames("x") %>%
as.matrix
assocs = as_cordf(assocs)
network_plot(assocs, min_cor = 0, colours = c("indianred2", "white", "steelblue4"))
library(tidyverse)
library(ggpubr)
library(shapr)
#library(ggforce)
library(caret)
library(ranger)
library(mgcv)
library(patchwork)
# Plots
source("extra/sina_plot.R")
source("extra/indiv_plot.R")
dataset <- read_csv("data/datasetADNI.csv")
data_train <- dataset[train_index,]
data_test <- dataset[-train_index,]
dataset <- read_csv("data/datasetADNI.csv")
head(dataset)
# Fixing covariables and response variable
x_var <- c("FDG","ABETA","PTAU","APOE4","PTGENDER","AGE","PTEDUCAT")
y_var <- "DXB" # Binary classification
# Splitting in train-test (80%-20%)
set.seed(2022)
train_index <- caret::createDataPartition(dataset$DXB, p = .8, list = FALSE, times = 1)
dataset <- read_csv("data/datasetADNI.csv")
data_train <- dataset[train_index,]
data_test <- dataset[-train_index,]
modelglm <- glm(DXB ~ ., data = data_train, family = binomial)
print(modelglm)
# Model evaluation
# Prediction
pred_test = predict(modelglm, data_test, type = "response")
# Converting to class: c = 0.5
pred_test[(pred_test > 0.5)] = 1
pred_test[(pred_test < 0.5)] = 0
# Creating confusion matrix
confusion = confusionMatrix(as.factor(c(y_test)), as.factor(pred_test))
print(confusion)
# Training data
x_train <- as.matrix(dataset[train_index, x_var])
y_train <- as.matrix(dataset[train_index, y_var])
# Test data
x_test <- as.matrix(dataset[-train_index, x_var])
y_test <- as.matrix(dataset[-train_index, y_var])
# Creating confusion matrix
confusion = confusionMatrix(as.factor(c(y_test)), as.factor(pred_test))
print(confusion)
# Model evaluation
# Prediction
pred_test = predict(modelglm, x_test, type = "response")
# x and y with factors
x_train_df <- dataset[train_index, x_var]
y_train_df <- dataset[train_index, y_var]
x_test_df <- dataset[-train_index, x_var]
y_test_df <- dataset[-train_index, y_var]
modelglm <- glm(DXB ~ ., data = data_train, family = binomial)
print(modelglm)
# Model evaluation
# Prediction
pred_test = predict(modelglm, x_test_df, type = "response")
# Converting to class: c = 0.5
pred_test[(pred_test > 0.5)] = 1
pred_test[(pred_test < 0.5)] = 0
# Creating confusion matrix
confusion = confusionMatrix(as.factor(c(y_test)), as.factor(pred_test))
print(confusion)
