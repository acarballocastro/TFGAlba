select(tvhours) %>% summarise(mean)
gss_cat %>%
summarise(promedio = mean(tvhours))
gss_cat %>%
summarise(promedio = mean(tvhours))
gss_cat %>%
summarise(promedio = mean(tvhours, na.rm = T))
gss_cat %>%
select(tvhours) %>%
summary
# Dibujamos la distribución
gss_cat %>%
filter(!is.na(tvhours)) %>%
ggplot(aes(x = tvhours)) +
geom_histogram(binwidth = 1)
# Dibujamos la distribución
gss_cat %>%
ggplot(aes(x = !is.na(tvhours))) +
geom_histogram(binwidth = 1)
# Dibujamos la distribución
gss_cat %>%
filter(!is.na(tvhours)) %>%
ggplot(aes(x = !is.na(tvhours))) +
geom_histogram(binwidth = 1)
# Dibujamos la distribución
gss_cat %>%
filter(!is.na(tvhours)) %>%
ggplot(aes(x = tvhours)) +
geom_histogram(binwidth = 1)
# Dibujamos la distribución
gss_cat %>%
filter(!is.na(tvhours)) %>% # Filtramos los NA
ggplot(aes(x = tvhours)) +
geom_histogram()
# Dibujamos la distribución
gss_cat %>%
filter(!is.na(tvhours)) %>% # Filtramos los NA
ggplot(aes(x = tvhours)) +
geom_histogram(bins = 24)
gss_cat %>% filter(partyid)
gss_cat %>% select(partyid)
gss_cat %>% select(partyid) %>%
levels
gss_cat %>% select(partyid) %>%
levels()
gss_cat %>% filter(!is.na(partyid)) %>%
count(partyid, sort = TRUE)
# Usamos fct_collapse para crear categorías
# Democrat: "Not str democrat", "Strong democrat"
# Republican: "Strong republican", "Not str republican"
# Independent: "Ind,near rep", "Independent", "Ind,near dem"
gss_cat %>%
# Usamos fct_collapse para crear categorías
# Democrat: "Not str democrat", "Strong democrat"
# Republican: "Strong republican", "Not str republican"
# Independent: "Ind,near rep", "Independent", "Ind,near dem"
gss_cat %>%
gss_cat %>%
mutate(partyid = fct_collapse(partyid, dem = c("Not str democrat", "Strong democrat"),
rep = c("Strong republican", "Not str republican"),
ind = c("Ind,near rep", "Independent", "Ind,near dem"))) %>%
count(year, partyid) %>% # Seleccionamos por año
group_by(year) %>% # Agurpamos por año
mutate(prop = n / sum(n)) %>% # Calculamos la proporción
ggplot(aes(
x = year, y = p,
colour = fct_reorder2(partyid, year, p)
)) +
geom_point() +
geom_line() +
labs(colour = "Party ID."
# Usamos fct_collapse para crear categorías
# Democrat: "Not str democrat", "Strong democrat"
# Republican: "Strong republican", "Not str republican"
# Independent: "Ind,near rep", "Independent", "Ind,near dem"
gss_cat %>%
mutate(partyid = fct_collapse(partyid, dem = c("Not str democrat", "Strong democrat"),
rep = c("Strong republican", "Not str republican"),
ind = c("Ind,near rep", "Independent", "Ind,near dem"))) %>%
count(year, partyid) %>% # Seleccionamos por año
group_by(year) %>% # Agurpamos por año
mutate(prop = n / sum(n)) %>% # Calculamos la proporción
ggplot(aes(
x = year, y = p,
colour = fct_reorder2(partyid, year, p)
)) +
geom_point() +
geom_line() +
labs(colour = "Party ID.")
install.packages("shinyWidgets")
shiny::runApp('C:/Users/albac/Desktop/MatEst/IAE/tarea_shiny_albcarcas1')
runApp('C:/Users/albac/Desktop/MatEst/IAE/tarea_shiny_albcarcas1')
runApp('C:/Users/albac/Desktop/MatEst/IAE/tarea_shiny_albcarcas1')
runApp('C:/Users/albac/Desktop/MatEst/IAE/tarea_shiny_albcarcas1')
runApp('C:/Users/albac/Desktop/MatEst/IAE/tarea_shiny_albcarcas1')
ruspini
data(ruspini)
library(ruspini)
library(hclust)
library(cluster)
ruspini
agriculture
# k-means only works with numerical variables,
# so don't give the user the option to select
# a categorical variable
data <- agriculture
runApp('C:/Users/albac/Desktop/MatEst/IAE/tarea_shiny_albcarcas1')
runApp('C:/Users/albac/Desktop/MatEst/IAE/tarea_shiny_albcarcas1')
USArrests
setdiff(names(iris), "Species")
names(USArrests)
runApp('C:/Users/albac/Desktop/MatEst/IAE/tarea_shiny_albcarcas1')
runApp('C:/Users/albac/Desktop/MatEst/IAE/tarea_shiny_albcarcas1')
runApp('C:/Users/albac/Desktop/MatEst/IAE/tarea_shiny_albcarcas1')
runApp('C:/Users/albac/Desktop/MatEst/IAE/tarea_shiny_albcarcas1')
runApp('C:/Users/albac/Desktop/MatEst/IAE/tarea_shiny_albcarcas1')
runApp('C:/Users/albac/Desktop/MatEst/IAE/tarea_shiny_albcarcas1')
plot(2:10, sapply(2:10,function(x) pam(USArrests,x)$silinfo$avg.width),
type = "l", xlab = "Number of medoids (k)",
ylab = "Average cluster width", main = "Average cluster width representation")
runApp('C:/Users/albac/Desktop/MatEst/IAE/tarea_shiny_albcarcas1')
runApp('C:/Users/albac/Desktop/MatEst/IAE/tarea_shiny_albcarcas1')
runApp('C:/Users/albac/Desktop/MatEst/IAE/tarea_shiny_albcarcas1')
pam = pam(data)
pam = pam(USArrests, 4)
pam$clustering
pam$clusinfo
runApp('C:/Users/albac/Desktop/MatEst/IAE/tarea_shiny_albcarcas1')
runApp('C:/Users/albac/Desktop/MatEst/IAE/tarea_shiny_albcarcas1')
runApp('C:/Users/albac/Desktop/MatEst/IAE/tarea_shiny_albcarcas1')
runApp('C:/Users/albac/Desktop/MatEst/IAE/tarea_shiny_albcarcas1')
runApp('C:/Users/albac/Desktop/MatEst/IAE/tarea_shiny_albcarcas1')
runApp('C:/Users/albac/Desktop/MatEst/IAE/tarea_shiny_albcarcas1/app2.R')
runApp('C:/Users/albac/Desktop/MatEst/IAE/tarea_shiny_albcarcas1/app3.R')
runApp('app3.R')
shiny::runApp('C:/Users/albac/Desktop/MatEst/IAE/tarea_shiny_albcarcas1')
install.packages("esquisse")
esquisse:::esquisser()
library(heplots)
data("RootStock")
summary(RootStock)
dim(RootStock)
R = cor(RootStock[,-1])
R
(autoval = eigen(R)$values)
round(autoval,2)
(autovec = eigen(R)$vectors)
(autoval = eigen(R)$values)
m = 2
L = autovec[,1:m]%*%diag(sqrt(autoval[1:m])) # Cargas factoriales
rownames(L)= colnames(RootStock[,-1])
colnames(L)= paste("Factor",1:m)
L # Matriz de cargas factoriales obtenida
(rotvarimax= varimax(L))
T = rotvarimax$rotmat # matriz de rotación
Lrot=unclass(rotvarimax$loadings)
#(Lrot= L%*%T)  #es lo mismo
colnames(Lrot)= c("F1 rot.","F2 rot.2")
Lrot
dataset <- read_csv("data/datasetADNI.csv")
library(tidyverse)
dataset <- read_csv("data/datasetADNI.csv")
setwd("C:/Users/albac/Desktop/MatEst/TFG/TFGAlba")
dataset <- read_csv("data/datasetADNI.csv")
head(dataset)
View(dataset)
sum(dataset$DXB)
library(tidyverse)
library(ggpubr)
library(rcompanion)
library(corrr)
library(shapr)
dataset <- read_csv("data/datasetADNI.csv")
dataset <- dataset[,c(-1,-9)] # Removing ID and DX variable
head(dataset)
mixed_assoc = function(df, cor_method="spearman", adjust_cramersv_bias=TRUE){
df_comb = expand.grid(names(df), names(df),  stringsAsFactors = F) %>% set_names("X1", "X2")
is_nominal = function(x) class(x) %in% c("factor", "character")
is_numeric <- function(x) { is.integer(x) || is_double(x)}
f = function(xName,yName) {
x =  pull(df, xName)
y =  pull(df, yName)
result = if(is_nominal(x) && is_nominal(y)){
# use bias corrected cramersV as described in https://rdrr.io/cran/rcompanion/man/cramerV.html
cv = cramerV(as.character(x), as.character(y), bias.correct = adjust_cramersv_bias)
data.frame(xName, yName, assoc=cv, type="cramersV")
}else if(is_numeric(x) && is_numeric(y)){
correlation = cor(x, y, method=cor_method, use="complete.obs")
data.frame(xName, yName, assoc=correlation, type="correlation")
}else if(is_numeric(x) && is_nominal(y)){
# from https://stats.stackexchange.com/questions/119835/correlation-between-a-nominal-iv-and-a-continuous-dv-variable/124618#124618
r_squared = summary(lm(x ~ y))$r.squared
data.frame(xName, yName, assoc=sqrt(r_squared), type="anova")
}else if(is_nominal(x) && is_numeric(y)){
r_squared = summary(lm(y ~x))$r.squared
data.frame(xName, yName, assoc=sqrt(r_squared), type="anova")
}else {
warning(paste("unmatched column type combination: ", class(x), class(y)))
}
# finally add complete obs number and ratio to table
result %>% mutate(complete_obs_pairs=sum(!is.na(x) & !is.na(y)), complete_obs_ratio=complete_obs_pairs/length(x)) %>% rename(x=xName, y=yName)
}
# apply function to each variable combination
map2_df(df_comb$X1, df_comb$X2, f)
}
mixed_assoc(dataset)
assocs <- dataset %>%
#select(-DXB) %>%
mixed_assoc() %>%
select(x, y, assoc) %>%
spread(y, assoc) %>%
column_to_rownames("x") %>%
as.matrix
assocs = as_cordf(assocs)
network_plot(assocs, min_cor = 0, colours = c("indianred2", "white", "steelblue4"))
library(tidyverse)
library(ggpubr)
library(shapr)
#library(ggforce)
library(caret)
library(ranger)
library(mgcv)
library(patchwork)
# Plots
source("extra/sina_plot.R")
source("extra/indiv_plot.R")
dataset <- read_csv("data/datasetADNI.csv")
data_train <- dataset[train_index,]
data_test <- dataset[-train_index,]
dataset <- read_csv("data/datasetADNI.csv")
head(dataset)
# Fixing covariables and response variable
x_var <- c("FDG","ABETA","PTAU","APOE4","PTGENDER","AGE","PTEDUCAT")
y_var <- "DXB" # Binary classification
# Splitting in train-test (80%-20%)
set.seed(2022)
train_index <- caret::createDataPartition(dataset$DXB, p = .8, list = FALSE, times = 1)
dataset <- read_csv("data/datasetADNI.csv")
data_train <- dataset[train_index,]
data_test <- dataset[-train_index,]
modelglm <- glm(DXB ~ ., data = data_train, family = binomial)
print(modelglm)
# Model evaluation
# Prediction
pred_test = predict(modelglm, data_test, type = "response")
# Converting to class: c = 0.5
pred_test[(pred_test > 0.5)] = 1
pred_test[(pred_test < 0.5)] = 0
# Creating confusion matrix
confusion = confusionMatrix(as.factor(c(y_test)), as.factor(pred_test))
print(confusion)
# Training data
x_train <- as.matrix(dataset[train_index, x_var])
y_train <- as.matrix(dataset[train_index, y_var])
# Test data
x_test <- as.matrix(dataset[-train_index, x_var])
y_test <- as.matrix(dataset[-train_index, y_var])
# Creating confusion matrix
confusion = confusionMatrix(as.factor(c(y_test)), as.factor(pred_test))
print(confusion)
# Model evaluation
# Prediction
pred_test = predict(modelglm, x_test, type = "response")
# x and y with factors
x_train_df <- dataset[train_index, x_var]
y_train_df <- dataset[train_index, y_var]
x_test_df <- dataset[-train_index, x_var]
y_test_df <- dataset[-train_index, y_var]
modelglm <- glm(DXB ~ ., data = data_train, family = binomial)
print(modelglm)
# Model evaluation
# Prediction
pred_test = predict(modelglm, x_test_df, type = "response")
# Converting to class: c = 0.5
pred_test[(pred_test > 0.5)] = 1
pred_test[(pred_test < 0.5)] = 0
# Creating confusion matrix
confusion = confusionMatrix(as.factor(c(y_test)), as.factor(pred_test))
print(confusion)
library(tidyverse)
library(ggpubr)
library(shapr)
#library(ggforce)
library(caret)
library(xgboost)
library(patchwork)
# Plots
source("extra/sina_plot.R")
source("extra/indiv_plot.R")
dataset <- read_csv("data/datasetADNI.csv")
head(dataset)
# Fixing covariables and response variable
x_var <- c("FDG","ABETA","PTAU","APOE4", "PTGENDER","AGE","PTEDUCAT")
y_var <- "DXB" # Binary classification
# Splitting in train-test (80%-20%)
set.seed(2022)
train_index <- caret::createDataPartition(dataset$DXB, p = .8, list = FALSE, times = 1)
# Training data
x_train <- as.matrix(dataset[train_index, x_var])
y_train <- as.matrix(dataset[train_index, y_var])
# Test data
x_test <- as.matrix(dataset[-train_index, x_var])
y_test <- as.matrix(dataset[-train_index, y_var])
hyper_tuning <- function(max_depth, min_child_weight, subsample) {
params = list(
objective="binary:logistic",
eval_metric="auc",
max_depth = max_depth,
min_child_weight = min_child_weight,
subsample = subsample
)
# 10-fold cross validation
xgbcv <- xgb.cv(params = params, data = x_train, label = y_train,
nrounds = 100, nfold = 10, prediction = T, showsd = T,
early.stop.round = 20, maximize = F, verbose = 0)
return(
list(
Score = max(xgbcv$evaluation_log$test_auc_mean)
, nrounds = xgbcv$best_iteration
)
)
}
library("ParBayesianOptimization")
params = list(
objective="binary:logistic",
eval_metric="error"
)
modelxgb <- xgboost(data = x_train, label = y_train, nround = 100,
verbose = FALSE, params = params)
print(modelxgb)
modelxgb <- xgboost(data = x_train, label = y_train, nround = 100,
verbose = FALSE, params = params)
print(modelxgb)
# Model evaluation
# Prediction
pred_test = predict(modelxgb, x_test)
# Converting to class: c = 0.5
pred_test[(pred_test >= 0.5)] = 1
pred_test[(pred_test < 0.5)] = 0
# Creating confusion matrix
confusion = confusionMatrix(as.factor(c(y_test)), as.factor(pred_test))
print(confusion)
## Dependent features ----
explainer <- shapr(x_train, modelxgb)
## Dependent features ----
explainer <- shapr(x_train, modelxgb)
p <- mean(y_train) # Expected prediction
### Gaussian ----
explanation_gaussian <- explain(x_test, approach = "gaussian",
explainer = explainer, prediction_zero = p,
seed = 2022)
explanation_gaussian
# Removing categorical variables
explanation_gaussian$x
# Removing categorical variables
explanation_gaussian$x[-c(4,5)]
names(explanation_gaussian$x)
explanation_gaussian$x
head(explanation_gaussian$x)
# Removing categorical variables
explanation_gaussian$x[-c(4,5)]
# Removing categorical variables
gaussian_cuant$x = explanation_gaussian$x[-c(4,5)]
# Removing categorical variables
gaussian_cuant = NULL
gaussian_cuant$x = explanation_gaussian$x[-c(4,5)]
explanation_gaussian
head(explanation_gaussian$x_test)
head(explanation_gaussian$x_test[-c(4,5)])
head(explanation_gaussian$x_test[,-c(4,5)])
# Removing categorical variables
gaussian_cuant = NULL
gaussian_cuant$x_test = explanation_gaussian$x_test[,-c(4,5)]
gaussian_cuant$dt = explanation_gaussian$dt[,-c(4,5)]
gaussian_cuant$p = explanation_gaussian$p[,-c(4,5)]
head(explanation_gaussian$p)
explanation_gaussian$p
gaussian_cuant$p = explanation_gaussian$p
sina_gaussian <- sina_plot(gaussian_cuant) +
ggtitle("Shapley values\nGaussian approach")
sina_gaussian
gaussian_cuant$dt
explanation_gaussian$dt
explanation_gaussian$x_test
### Gaussian ----
explanation_gaussian <- explain(x_test, approach = "gaussian",
explainer = explainer, prediction_zero = p,
seed = 2022)
# Removing categorical variables
gaussian_cuant = NULL
explanation_gaussian$x_test[,-c(4,5)]
explanation_gaussian$dt[,-c(4,5)]
explanation_gaussian$dt
# Removing categorical variables
gaussian_cuant = NULL
gaussian_cuant$x_test = explanation_gaussian$x_test[,-c(4,5)]
gaussian_cuant$dt = explanation_gaussian$dt[,-c(5,6)]
gaussian_cuant$p = explanation_gaussian$p
sina_gaussian <- sina_plot(gaussian_cuant) +
ggtitle("Shapley values\nGaussian approach")
sina_gaussian
### Copula ----
explanation_copula <- explain(x_test, approach = "copula",
explainer = explainer, prediction_zero = p,
seed = 2022)
# Removing categorical variables
copula_cuant = NULL
copula_cuant$x_test = explanation_copula$x_test[,-c(4,5)]
copula_cuant$dt = explanation_copula$dt[,-c(5,6)]
copula_cuant$p = explanation_copula$p
sina_copula <- sina_plot(copula_cuant) +
coord_flip(ylim = ylim_gaussian) + ggtitle("Shapley values\nCopula approach")
# save limits of sina_gaussian plot for comparing against marginal and asymmetric
ylim_gaussian <- sina_gaussian$coordinates$limits$y
sina_copula <- sina_plot(copula_cuant) +
coord_flip(ylim = ylim_gaussian) + ggtitle("Shapley values\nCopula approach")
sina_copula
print(confusion)
levels(DXB)
levels(datos$DXB)
levels(dataset$DXB)
dataset$DXB
dataset <- read_csv("data/datasetADNI.csv")
data_train <- dataset[train_index,]
data_test <- dataset[-train_index,]
# x and y with factors
x_train_df <- dataset[train_index, x_var]
y_train_df <- dataset[train_index, y_var]
x_test_df <- dataset[-train_index, x_var]
y_test_df <- dataset[-train_index, y_var]
modelglm <- glm(DXB ~ ., data = data_train, family = binomial)
print(modelglm)
# Model evaluation
# Prediction
pred_test = predict(modelglm, x_test_df, type = "response")
# Converting to class: c = 0.5
pred_test[(pred_test > 0.5)] = 1
pred_test[(pred_test < 0.5)] = 0
# Creating confusion matrix
confusion = confusionMatrix(as.factor(c(y_test)), as.factor(pred_test))
print(confusion)
dataset <- read_csv("data/datasetADNI.csv")
head(dataset)
# Fixing covariables and response variable
x_var <- c("FDG","ABETA","PTAU","APOE4","PTGENDER","AGE","PTEDUCAT")
y_var <- "DXB" # Binary classification
# Splitting in train-test (80%-20%)
set.seed(2022)
train_index <- caret::createDataPartition(dataset$DXB, p = .8, list = FALSE, times = 1)
# Training data
x_train <- as.matrix(dataset[train_index, x_var])
y_train <- as.matrix(dataset[train_index, y_var])
# Test data
x_test <- as.matrix(dataset[-train_index, x_var])
y_test <- as.matrix(dataset[-train_index, y_var])
# Factor variables converted
# dataset$APOE4 = as.factor(dataset$APOE4)
# dataset$PTGENDER = as.factor(dataset$PTGENDER)
dataset$DXB = as.factor(dataset$DXB)
head(dataset)
levels(dataset$DXB)
data1 = read_csv("data/ADNIMERGE_May15.2014.csv")
data2 = read_csv("data/UPENN_CSF Biomarkers_baseline_May15.2014.csv")
data1 %<>%
filter(VISCODE == "bl")
## Combining both datasets and selecting relevant variables
dataset <- inner_join(data1, data2, by = "RID") %>%
select(RID, FDG, ABETA, PTAU, APOE4, PTGENDER, AGE, PTEDUCAT, DX.bl) %>%
na.omit() %>%
mutate(APOE4 = as.factor(APOE4),
PTGENDER = as.factor(PTGENDER),
DX = as.factor(DX.bl)) %>%
mutate(DX = fct_collapse(DX, MCI = c("EMCI", "LMCI")),
DXB = fct_collapse(DX, DP = c("MCI", "AD"))) %>%
select(-DX.bl)
## Coding factors
dataset$PTGENDER = unclass(dataset$PTGENDER)-1
dataset$PTGENDER = as.factor(dataset$PTGENDER)
dataset$DX = unclass(dataset$DX)-1
dataset$DX = as.factor(dataset$DX)
dataset$DXB = unclass(dataset$DXB)-1
dataset$DXB = as.factor(dataset$DXB)
head(dataset)
levels(dataset$DXB)
## Combining both datasets and selecting relevant variables
dataset <- inner_join(data1, data2, by = "RID") %>%
select(RID, FDG, ABETA, PTAU, APOE4, PTGENDER, AGE, PTEDUCAT, DX.bl) %>%
na.omit() %>%
mutate(APOE4 = as.factor(APOE4),
PTGENDER = as.factor(PTGENDER),
DX = as.factor(DX.bl)) %>%
mutate(DX = fct_collapse(DX, MCI = c("EMCI", "LMCI")),
DXB = fct_collapse(DX, DP = c("MCI", "AD"))) %>%
select(-DX.bl)
levels(dataset$DXB)
head(dataset)
dataset <- read_csv("data/datasetADNI.csv")
head(dataset)
## Combining both datasets and selecting relevant variables
dataset <- inner_join(data1, data2, by = "RID") %>%
select(RID, FDG, ABETA, PTAU, APOE4, PTGENDER, AGE, PTEDUCAT, DX.bl) %>%
na.omit() %>%
mutate(APOE4 = as.factor(APOE4),
PTGENDER = as.factor(PTGENDER),
DX = as.factor(DX.bl)) %>%
mutate(DX = fct_collapse(DX, MCI = c("EMCI", "LMCI")),
DXB = fct_collapse(DX, DP = c("MCI", "AD"))) %>%
select(-DX.bl)
## Coding factors
dataset$PTGENDER = unclass(dataset$PTGENDER)-1
dataset$PTGENDER = as.factor(dataset$PTGENDER)
levels(dataset$DXB)
