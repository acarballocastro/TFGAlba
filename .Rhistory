summarise(num_vuelos = n(),
dist_media = mean(distance, na.rm = TRUE),
ret_medio = mean(dep_delay, na.rm = TRUE)
) %>%
filter(num_vuelos > 20, dest != "HNL"))
retraso_salida %>%
ggplot(aes(x = dist_media, y = ret_medio)) +
geom_point(aes(size = num_vuelos), alpha = 1/3) +
geom_smooth(se = FALSE)
(retraso_salida <- flights %>%
group_by(dest) %>%
summarise(num_vuelos = n(),
dist_media = mean(distance, na.rm = TRUE),
ret_medio_salida = mean(dep_delay, na.rm = TRUE)
) %>%
filter(num_vuelos > 20, dest != "HNL"))
retraso_salida %>%
ggplot(aes(x = dist_media, y = ret_medio_salida)) +
geom_point(aes(size = num_vuelos), alpha = 1/3) +
geom_smooth(se = FALSE)
library(tidyverse)
library(forecats) # Dentro de tidyverse
library(forcats) # Dentro de tidyverse
gss_cat
head(gss_cat) # Datos
gss_cat %>%
count(relig) %>% # Hacemos que cuente las religiones
arrange(desc(n)) %>% # Ordenamos descendentemente
head(1)
gss_cat %>%
count(relig) %>% # Hacemos que cuente las religiones
arrange(desc(n)) %>% # Ordenamos descendentemente
first()
gss_cat %>%
count(relig) %>% # Hacemos que cuente las religiones
arrange(desc(n))
gss_cat %>%
count(relig) %>% # Hacemos que cuente las religiones
arrange(desc(n)) %>% # Ordenamos descendentemente
slice_head()
gss_cat %>%
count(relig) %>% # Hacemos que cuente las religiones
arrange(desc(n)) %>% # Ordenamos descendentemente
slice_head() # head(1) también habría valido
gss_cat %>%
count(partyid) %>% # Hacemos que cuente las religiones
arrange(desc(n)) %>% # Ordenamos descendentemente
slice_head()
gss_cat <- gss_cat %>%
select_if(is.factor)
gss_cat %>%
select_if(is.factor)
head(gss_cat) # Datos
library(tidyverse) # forcats esta dentro
head(gss_cat) # Datos
library(tidyverse) # forcats esta dentro
head(gss_cat) # Datos
library(tidyverse) # forcats esta dentro
head(gss_cat) # Datos
gss_cat %>%
select_if(is.factor)
gss_cat %>%
select_if(is.factor) %>%
colnames()
# Marital
levels(gss_cat[["marital"]])
gss_cat %>%
select(tvhours) %>%
summary
head(gss_cat) # Datos
str(gss_cat)
gss_cat %>%
select(tvhours) %>%
summary
gss_cat %>%
select(tvhours) %>% mean
gss_cat %>%
select(tvhours) %>% summarise(mean)
gss_cat %>%
summarise(promedio = mean(tvhours))
gss_cat %>%
summarise(promedio = mean(tvhours))
gss_cat %>%
summarise(promedio = mean(tvhours, na.rm = T))
gss_cat %>%
select(tvhours) %>%
summary
# Dibujamos la distribución
gss_cat %>%
filter(!is.na(tvhours)) %>%
ggplot(aes(x = tvhours)) +
geom_histogram(binwidth = 1)
# Dibujamos la distribución
gss_cat %>%
ggplot(aes(x = !is.na(tvhours))) +
geom_histogram(binwidth = 1)
# Dibujamos la distribución
gss_cat %>%
filter(!is.na(tvhours)) %>%
ggplot(aes(x = !is.na(tvhours))) +
geom_histogram(binwidth = 1)
# Dibujamos la distribución
gss_cat %>%
filter(!is.na(tvhours)) %>%
ggplot(aes(x = tvhours)) +
geom_histogram(binwidth = 1)
# Dibujamos la distribución
gss_cat %>%
filter(!is.na(tvhours)) %>% # Filtramos los NA
ggplot(aes(x = tvhours)) +
geom_histogram()
# Dibujamos la distribución
gss_cat %>%
filter(!is.na(tvhours)) %>% # Filtramos los NA
ggplot(aes(x = tvhours)) +
geom_histogram(bins = 24)
gss_cat %>% filter(partyid)
gss_cat %>% select(partyid)
gss_cat %>% select(partyid) %>%
levels
gss_cat %>% select(partyid) %>%
levels()
gss_cat %>% filter(!is.na(partyid)) %>%
count(partyid, sort = TRUE)
# Usamos fct_collapse para crear categorías
# Democrat: "Not str democrat", "Strong democrat"
# Republican: "Strong republican", "Not str republican"
# Independent: "Ind,near rep", "Independent", "Ind,near dem"
gss_cat %>%
# Usamos fct_collapse para crear categorías
# Democrat: "Not str democrat", "Strong democrat"
# Republican: "Strong republican", "Not str republican"
# Independent: "Ind,near rep", "Independent", "Ind,near dem"
gss_cat %>%
gss_cat %>%
mutate(partyid = fct_collapse(partyid, dem = c("Not str democrat", "Strong democrat"),
rep = c("Strong republican", "Not str republican"),
ind = c("Ind,near rep", "Independent", "Ind,near dem"))) %>%
count(year, partyid) %>% # Seleccionamos por año
group_by(year) %>% # Agurpamos por año
mutate(prop = n / sum(n)) %>% # Calculamos la proporción
ggplot(aes(
x = year, y = p,
colour = fct_reorder2(partyid, year, p)
)) +
geom_point() +
geom_line() +
labs(colour = "Party ID."
# Usamos fct_collapse para crear categorías
# Democrat: "Not str democrat", "Strong democrat"
# Republican: "Strong republican", "Not str republican"
# Independent: "Ind,near rep", "Independent", "Ind,near dem"
gss_cat %>%
mutate(partyid = fct_collapse(partyid, dem = c("Not str democrat", "Strong democrat"),
rep = c("Strong republican", "Not str republican"),
ind = c("Ind,near rep", "Independent", "Ind,near dem"))) %>%
count(year, partyid) %>% # Seleccionamos por año
group_by(year) %>% # Agurpamos por año
mutate(prop = n / sum(n)) %>% # Calculamos la proporción
ggplot(aes(
x = year, y = p,
colour = fct_reorder2(partyid, year, p)
)) +
geom_point() +
geom_line() +
labs(colour = "Party ID.")
install.packages("shinyWidgets")
shiny::runApp('C:/Users/albac/Desktop/MatEst/IAE/tarea_shiny_albcarcas1')
runApp('C:/Users/albac/Desktop/MatEst/IAE/tarea_shiny_albcarcas1')
runApp('C:/Users/albac/Desktop/MatEst/IAE/tarea_shiny_albcarcas1')
runApp('C:/Users/albac/Desktop/MatEst/IAE/tarea_shiny_albcarcas1')
runApp('C:/Users/albac/Desktop/MatEst/IAE/tarea_shiny_albcarcas1')
ruspini
data(ruspini)
library(ruspini)
library(hclust)
library(cluster)
ruspini
agriculture
# k-means only works with numerical variables,
# so don't give the user the option to select
# a categorical variable
data <- agriculture
runApp('C:/Users/albac/Desktop/MatEst/IAE/tarea_shiny_albcarcas1')
runApp('C:/Users/albac/Desktop/MatEst/IAE/tarea_shiny_albcarcas1')
USArrests
setdiff(names(iris), "Species")
names(USArrests)
runApp('C:/Users/albac/Desktop/MatEst/IAE/tarea_shiny_albcarcas1')
runApp('C:/Users/albac/Desktop/MatEst/IAE/tarea_shiny_albcarcas1')
runApp('C:/Users/albac/Desktop/MatEst/IAE/tarea_shiny_albcarcas1')
runApp('C:/Users/albac/Desktop/MatEst/IAE/tarea_shiny_albcarcas1')
runApp('C:/Users/albac/Desktop/MatEst/IAE/tarea_shiny_albcarcas1')
runApp('C:/Users/albac/Desktop/MatEst/IAE/tarea_shiny_albcarcas1')
plot(2:10, sapply(2:10,function(x) pam(USArrests,x)$silinfo$avg.width),
type = "l", xlab = "Number of medoids (k)",
ylab = "Average cluster width", main = "Average cluster width representation")
runApp('C:/Users/albac/Desktop/MatEst/IAE/tarea_shiny_albcarcas1')
runApp('C:/Users/albac/Desktop/MatEst/IAE/tarea_shiny_albcarcas1')
runApp('C:/Users/albac/Desktop/MatEst/IAE/tarea_shiny_albcarcas1')
pam = pam(data)
pam = pam(USArrests, 4)
pam$clustering
pam$clusinfo
runApp('C:/Users/albac/Desktop/MatEst/IAE/tarea_shiny_albcarcas1')
runApp('C:/Users/albac/Desktop/MatEst/IAE/tarea_shiny_albcarcas1')
runApp('C:/Users/albac/Desktop/MatEst/IAE/tarea_shiny_albcarcas1')
runApp('C:/Users/albac/Desktop/MatEst/IAE/tarea_shiny_albcarcas1')
runApp('C:/Users/albac/Desktop/MatEst/IAE/tarea_shiny_albcarcas1')
runApp('C:/Users/albac/Desktop/MatEst/IAE/tarea_shiny_albcarcas1/app2.R')
runApp('C:/Users/albac/Desktop/MatEst/IAE/tarea_shiny_albcarcas1/app3.R')
runApp('app3.R')
shiny::runApp('C:/Users/albac/Desktop/MatEst/IAE/tarea_shiny_albcarcas1')
install.packages("esquisse")
esquisse:::esquisser()
library(heplots)
data("RootStock")
summary(RootStock)
dim(RootStock)
R = cor(RootStock[,-1])
R
(autoval = eigen(R)$values)
round(autoval,2)
(autovec = eigen(R)$vectors)
(autoval = eigen(R)$values)
m = 2
L = autovec[,1:m]%*%diag(sqrt(autoval[1:m])) # Cargas factoriales
rownames(L)= colnames(RootStock[,-1])
colnames(L)= paste("Factor",1:m)
L # Matriz de cargas factoriales obtenida
(rotvarimax= varimax(L))
T = rotvarimax$rotmat # matriz de rotación
Lrot=unclass(rotvarimax$loadings)
#(Lrot= L%*%T)  #es lo mismo
colnames(Lrot)= c("F1 rot.","F2 rot.2")
Lrot
library(tidyverse)
library(ggpubr)
library(rcompanion)
library(corrr)
library(shapr)
#library(ggforce)
library(caret)
library(xgboost)
library(patchwork)
dataset <- read_csv("data/datasetADNI.csv")
dataset <- dataset[,c(-1,-9)] # Removing ID and DX variable
head(dataset)
mixed_assoc = function(df, cor_method="spearman", adjust_cramersv_bias=TRUE){
df_comb = expand.grid(names(df), names(df),  stringsAsFactors = F) %>% set_names("X1", "X2")
is_nominal = function(x) class(x) %in% c("factor", "character")
is_numeric <- function(x) { is.integer(x) || is_double(x)}
f = function(xName,yName) {
x =  pull(df, xName)
y =  pull(df, yName)
result = if(is_nominal(x) && is_nominal(y)){
# use bias corrected cramersV as described in https://rdrr.io/cran/rcompanion/man/cramerV.html
cv = cramerV(as.character(x), as.character(y), bias.correct = adjust_cramersv_bias)
data.frame(xName, yName, assoc=cv, type="cramersV")
}else if(is_numeric(x) && is_numeric(y)){
correlation = cor(x, y, method=cor_method, use="complete.obs")
data.frame(xName, yName, assoc=correlation, type="correlation")
}else if(is_numeric(x) && is_nominal(y)){
# from https://stats.stackexchange.com/questions/119835/correlation-between-a-nominal-iv-and-a-continuous-dv-variable/124618#124618
r_squared = summary(lm(x ~ y))$r.squared
data.frame(xName, yName, assoc=sqrt(r_squared), type="anova")
}else if(is_nominal(x) && is_numeric(y)){
r_squared = summary(lm(y ~x))$r.squared
data.frame(xName, yName, assoc=sqrt(r_squared), type="anova")
}else {
warning(paste("unmatched column type combination: ", class(x), class(y)))
}
# finally add complete obs number and ratio to table
result %>% mutate(complete_obs_pairs=sum(!is.na(x) & !is.na(y)), complete_obs_ratio=complete_obs_pairs/length(x)) %>% rename(x=xName, y=yName)
}
# apply function to each variable combination
map2_df(df_comb$X1, df_comb$X2, f)
}
mixed_assoc(dataset)
dataset <- read_csv("data/datasetADNI.csv")
setwd("C:/Users/albac/Desktop/MatEst/TFG/TFGAlba")
dataset <- read_csv("data/datasetADNI.csv")
dataset <- dataset[,c(-1,-9)] # Removing ID and DX variable
head(dataset)
mixed_assoc = function(df, cor_method="spearman", adjust_cramersv_bias=TRUE){
df_comb = expand.grid(names(df), names(df),  stringsAsFactors = F) %>% set_names("X1", "X2")
is_nominal = function(x) class(x) %in% c("factor", "character")
is_numeric <- function(x) { is.integer(x) || is_double(x)}
f = function(xName,yName) {
x =  pull(df, xName)
y =  pull(df, yName)
result = if(is_nominal(x) && is_nominal(y)){
# use bias corrected cramersV as described in https://rdrr.io/cran/rcompanion/man/cramerV.html
cv = cramerV(as.character(x), as.character(y), bias.correct = adjust_cramersv_bias)
data.frame(xName, yName, assoc=cv, type="cramersV")
}else if(is_numeric(x) && is_numeric(y)){
correlation = cor(x, y, method=cor_method, use="complete.obs")
data.frame(xName, yName, assoc=correlation, type="correlation")
}else if(is_numeric(x) && is_nominal(y)){
# from https://stats.stackexchange.com/questions/119835/correlation-between-a-nominal-iv-and-a-continuous-dv-variable/124618#124618
r_squared = summary(lm(x ~ y))$r.squared
data.frame(xName, yName, assoc=sqrt(r_squared), type="anova")
}else if(is_nominal(x) && is_numeric(y)){
r_squared = summary(lm(y ~x))$r.squared
data.frame(xName, yName, assoc=sqrt(r_squared), type="anova")
}else {
warning(paste("unmatched column type combination: ", class(x), class(y)))
}
# finally add complete obs number and ratio to table
result %>% mutate(complete_obs_pairs=sum(!is.na(x) & !is.na(y)), complete_obs_ratio=complete_obs_pairs/length(x)) %>% rename(x=xName, y=yName)
}
# apply function to each variable combination
map2_df(df_comb$X1, df_comb$X2, f)
}
mixed_assoc(dataset)
assocs <- dataset %>%
#select(-DXB) %>%
mixed_assoc() %>%
select(x, y, assoc) %>%
spread(y, assoc) %>%
column_to_rownames("x") %>%
as.matrix
assocs = as_cordf(assocs)
network_plot(assocs, min_cor = 0, colours = c("indianred2", "white", "steelblue4"))
network_plot(assocs, min_cor = 0, colours = c("indianred2", "white", "steelblue4"),
title = "Network Plot ADNI")
library(tidyverse)
library(ggpubr)
library(shapr)
#library(ggforce)
library(caret)
library(xgboost)
library(patchwork)
# Plots
source("extra/sina_plot.R")
source("extra/indiv_plot.R")
dataset <- read_csv("data/datasetADNI.csv")
head(dataset)
# Fixing covariables and response variable
x_var <- c("FDG","ABETA","PTAU","APOE4_0","APOE4_1", "PTGENDER","AGE","PTEDUCAT")
y_var <- "DXB" # Binary classification
# Splitting in train-test (80%-20%)
set.seed(2022)
train_index <- caret::createDataPartition(dataset$DXB, p = .8, list = FALSE, times = 1)
# Training data
x_train <- as.matrix(dataset[train_index, x_var])
y_train <- as.matrix(dataset[train_index, y_var])
# Test data
x_test <- as.matrix(dataset[-train_index, x_var])
y_test <- as.matrix(dataset[-train_index, y_var])
# Parameters: binary/logistic classification (supported by shapr)
params = list(
objective="binary:logistic",
eval_metric="error"
)
xgbcv <- xgb.cv(params = params, data = x_train, label = y_train,
nrounds = 100, nfold = 5, showsd = T, stratified = T,
print.every.n = 10, early.stop.round = 20, maximize = F)
modelxgb <- xgboost(data = x_train, label = y_train, nround = 100,
verbose = FALSE, params = params)
print(modelxgb)
# Model evaluation
# Prediction
pred_test = predict(modelxgb, x_test)
# Converting to class: c = 0.5
pred_test[(pred_test >= 0.4)] = 1
pred_test[(pred_test < 0.4)] = 0
# Creating confusion matrix
confusion = confusionMatrix(as.factor(c(y_test)), as.factor(pred_test))
print(confusion)
# Model evaluation
# Prediction
pred_test = predict(modelxgb, x_test)
# Converting to class: c = 0.5
pred_test[(pred_test >= 0.5)] = 1
pred_test[(pred_test < 0.5)] = 0
# Creating confusion matrix
confusion = confusionMatrix(as.factor(c(y_test)), as.factor(pred_test))
print(confusion)
## Dependent features ----
explainer <- shapr(x_train, modelxgb)
p <- mean(y_train) # Expected prediction
### Gaussian ----
explanation_gaussian <- explain(x_test, approach = "gaussian",
explainer = explainer, prediction_zero = p,
seed = 2022)
sina_gaussian <- sina_plot(explanation_gaussian) +
ggtitle("Shapley values\nGaussian approach")
# save limits of sina_gaussian plot for comparing against marginal and asymmetric
ylim_gaussian <- sina_gaussian$coordinates$limits$y
sina_gaussian
library(tidyverse)
library(magrittr)
library(cluster)
data1 = read_csv("data/ADNIMERGE_May15.2014.csv")
data2 = read_csv("data/UPENN_CSF Biomarkers_baseline_May15.2014.csv")
data1 %<>%
filter(VISCODE == "bl")
## Combining both datasets and selecting relevant variables
dataset <- inner_join(data1, data2, by = "RID") %>%
select(RID, FDG, ABETA, PTAU, APOE4, PTGENDER, AGE, PTEDUCAT, DX.bl) %>%
na.omit() %>%
mutate(APOE4 = as.factor(APOE4),
PTGENDER = as.factor(PTGENDER),
DX = as.factor(DX.bl)) %>%
mutate(DX = fct_collapse(DX, MCI = c("EMCI", "LMCI")),
DXB = fct_collapse(DX, DP = c("MCI", "AD"))) %>%
select(-DX.bl)
## Coding factors
dataset$PTGENDER = unclass(dataset$PTGENDER)-1
dataset$PTGENDER = as.factor(dataset$PTGENDER)
dataset$DX = unclass(dataset$DX)-1
dataset$DX = as.factor(dataset$DX)
dataset$DXB = unclass(dataset$DXB)-1
dataset$DXB = as.factor(dataset$DXB)
head(dataset)
str(dataset)
summary(dataset)
dataset %>%
select_if(is.numeric) %>%
map(sd)
dataset %>%
select_if(is.factor) %>%
map(table) %>% map(prop.table)
head(dataset)
# We remove the columns RID and DX
dataset = dataset[-c(1,9)]
# Saving dataset for further use
write_csv(dataset, "data/datasetADNI.csv")
# Plots
source("extra/sina_plot.R")
source("extra/indiv_plot.R")
dataset <- read_csv("data/datasetADNI.csv")
head(dataset)
# Fixing covariables and response variable
x_var <- c("FDG","ABETA","PTAU","APOE4", "PTGENDER","AGE","PTEDUCAT")
y_var <- "DXB" # Binary classification
# Splitting in train-test (80%-20%)
set.seed(2022)
train_index <- caret::createDataPartition(dataset$DXB, p = .8, list = FALSE, times = 1)
# Training data
x_train <- as.matrix(dataset[train_index, x_var])
y_train <- as.matrix(dataset[train_index, y_var])
# Test data
x_test <- as.matrix(dataset[-train_index, x_var])
y_test <- as.matrix(dataset[-train_index, y_var])
# Parameters: binary/logistic classification (supported by shapr)
params = list(
objective="binary:logistic",
eval_metric="error"
)
xgbcv <- xgb.cv(params = params, data = x_train, label = y_train,
nrounds = 100, nfold = 5, showsd = T, stratified = T,
print.every.n = 10, early.stop.round = 20, maximize = F)
modelxgb <- xgboost(data = x_train, label = y_train, nround = 100,
verbose = FALSE, params = params)
print(modelxgb)
# Model evaluation
# Prediction
pred_test = predict(modelxgb, x_test)
# Converting to class: c = 0.5
pred_test[(pred_test >= 0.5)] = 1
pred_test[(pred_test < 0.5)] = 0
# Creating confusion matrix
confusion = confusionMatrix(as.factor(c(y_test)), as.factor(pred_test))
print(confusion)
## Dependent features ----
explainer <- shapr(x_train, modelxgb)
p <- mean(y_train) # Expected prediction
### Gaussian ----
explanation_gaussian <- explain(x_test, approach = "gaussian",
explainer = explainer, prediction_zero = p,
seed = 2022)
sina_gaussian <- sina_plot(explanation_gaussian) +
ggtitle("Shapley values\nGaussian approach")
# save limits of sina_gaussian plot for comparing against marginal and asymmetric
ylim_gaussian <- sina_gaussian$coordinates$limits$y
sina_gaussian
### Copula ----
explanation_copula <- explain(x_test, approach = "copula",
explainer = explainer, prediction_zero = p,
seed = 2022)
sina_copula <- sina_plot(explanation_copula) +
coord_flip(ylim = ylim_gaussian) + ggtitle("Shapley values\nCopula approach")
sina_copula
### Empirical ----
explanation_empirical <- explain(x_test, approach = "empirical",
explainer = explainer, prediction_zero = p,
seed = 2022)
sina_empirical <- sina_plot(explanation_empirical) +
coord_flip(ylim = ylim_gaussian) + ggtitle("Shapley values\nEmpirical approach")
sina_empirical
explainer_symmetric <- shapr(x_train, modelxgb)
p <- mean(y_train) # Expected prediction
### Causal Shapley values ----
partial_order <- list(c(5,4,6,7), c(2), c(1,3))
explanation_causal <- explain(x_test, approach = "causal",
explainer = explainer_symmetric,
prediction_zero = p, ordering = partial_order,
confounding = c(TRUE, FALSE, TRUE), seed = 2022)
sina_causal <- sina_plot(explanation_causal) +
coord_flip(ylim = ylim_gaussian)
sina_causal
### Marginal Shapley values ----
# Assumes one component with confounding
explanation_marginal <- explain(x_test, approach = "causal",
explainer = explainer_symmetric,
prediction_zero = p, ordering = list(c(1:7)),
confounding = TRUE, seed = 2020)
sina_marginal <- sina_plot(explanation_marginal) +
coord_flip(ylim = ylim_gaussian) + ggtitle("Marginal Shapley values")
sina_marginal
